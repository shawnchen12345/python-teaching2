# 零基础大模型应用实战 - 讲师授课脚本 (v2.0 应用版)

**核心调整**:
不再纠结于复杂的 PyTorch 训练代码，而是教大家如何**驾驭**这些强大的开源模型，快速搭建出像 Google NotebookLM 或 AI 搜索 这样的应用。

---

## 👋 开场白 (10分钟)

**Hook (吸引点)**:
"大家好！现在拥有一个大模型并不难，难的是如何让它为你所用。
今天我们不当‘造车的人’（那是 Google 和 OpenAI 的事），我们要当‘赛车手’。
在这 8 小时里，我们要学会：即便只有普通电脑，也能跑起堪比 GPT-3.5 的本地模型，并且让它阅读你的私密文档，成为你的全能助手。"

**课程目标**:
1.  **跑起来**: 本地部署 Quantized (量化) 模型。
2.  **用得好**: Prompt Engineering (提示词工程) + Chain (链式调用)。
3.  **接外脑**: RAG (检索增强生成) —— 让模型读懂它没学过的书。

---

## 🏗️ 第一阶段：模型部署与服务化 (2.5小时)

### 1.1 把大象请回家：Quantization (量化) (1小时)
**概念**:
"现在的模型虽然强，但也太大。7B 模型原本像个 14G 的大胖子。
**量化 (Quantization)** 技术就像脱水蔬菜。把 32位的浮点数 (FP32) 变成 4位的整数 (INT4)。
*   体积：缩小 8 倍 -> 变成 2G 左右的小瘦子。
*   效果：几乎没变 (只变傻了一点点)。
*   好处：你的笔记本也能跑了！"

**实操**:
1.  **Library**: 安装 `bitsandbytes`, `accelerate`。
2.  **Download**: 从 Hugging Face / ModelScope 下载 `Qwen1.5-7B-Chat-Int4`。
3.  **Run**:
    ```python
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen1.5-7B-Chat-Int4",
        device_map="auto" # 自动把模型切分塞进显卡
    )
    ```
    *   *Check Point*: 成功对话，且显存占用极低。

### 1.2 给大象装电话：API 服务 (1.5小时)
**场景**:
"模型现在只能在 Python 脚本里和你对话。但如果你想用网页、用 Excel、用微信小程序调用它怎么办？"
**解决方案**: FastAPI。

**架构**:
`User` -> `HTTP Request (POST)` -> `FastAPI Server` -> `LLM` -> `Response`

**代码演示**:
编写一个 `server.py`，启动后，任何程序只需发送 JSON 数据包：
`{"prompt": "你好", "history": []}`
就能收到 AI 的回复。这是所有商业 AI 应用的基础。

---

## 🧠 第二阶段：提示词工程与逻辑构建 (2.5小时)

### 2.1 咒语课：System Prompt & CoT (1小时)
**比喻**:
"模型就像个天才实习生，但它需要明确的指令。
*   **System Prompt**: 角色设定。‘你是苏格拉底’ vs ‘你是严厉的面试官’。
*   **Chain of Thought (CoT)**: 思维链。
    *   *Bad Prompt*: ‘鸡兔同笼，35头94脚，几只鸡？’ -> 模型可能乱猜。
    *   *Good Prompt*: ‘请一步步思考：1. 假设全是鸡... 2. 多出来的脚...’ -> 模型就能算对。"

### 2.2 让模型听话：结构化输出 (1.5小时)
**痛点**:
"我想让 AI 帮我从新闻里提取人名和地点，存进 Excel。但模型总喜欢说废话：‘好的，这是提取结果...’"
**实操**:
强迫模型输出 JSON。
Prompt:
`你是一个数据提取器。只输出 JSON 对象，不要包含任何其他文字。格式：{"name": "xxx", "location": "xxx"}`
这让我们可以直接用 `json.loads()` 解析结果，实现自动化流程。

---

## 📚 第三阶段：RAG - 给大象装上外脑 (3小时)

这是目前最火的技术方向。

### 3.1 为什么需要 RAG (检索增强生成)？(30分钟)
**问题**:
"模型是去年训练的，它不知道今天的股市，也不知道你们公司的《员工手册》。"
**错误做法**: 把整本书塞进 Prompt 里 (太长，塞不下)。
**正确做法 (RAG)**:
"考试时允许开卷。
1.  用户问：‘迟到扣多少钱？’
2.  **检索系统**: 去《员工手册》里翻书，找到第 5 页第 2 段关于迟到的规定。
3.  **生成**: 把用户的问题 + 刚才找到的规定，一起发给模型。
4.  模型回答：‘根据规定，迟到扣 50。’"

### 3.2 向量数据库 (Vector DB) 原理 (1小时)
**核心**: Embedding (嵌入)。
"电脑不认识字，只认识数字。
Embedding 模型把一句话变成一串数字 (向量)。
*   ‘苹果’的向量和‘水果’离得很近。
*   ‘苹果’的向量和‘手机’离得很远。
这样我们就能通过数学计算，快速找出和用户问题最相关的文档片段。"

### 3.3 实战：构建私有知识库助手 (1.5小时)
**流程**:
1.  **Ingest**: 上传 PDF -> 切分 (Chunking) -> 向量化 (Embedding) -> 存入 ChromaDB。
2.  **Retrieve**: 用户提问 -> 向量化 -> 在数据库中搜索 Top-3 相关片段。
3.  **Generation**:
    Prompt = `基于以下资料回答问题：\n---\n{context}\n---\n问题：{question}`
    LLM -> 最终答案。

**大作业**:
使用 `Streamlit` 做前端，`LangChain` 做逻辑，`Qwen-7B` 做大脑，搭建一个 "论文阅读助手"。上传 PDF，立刻就能问它论文里的细节。
